# Memory Management Policy
### Beyond Physical Memory: Mechanisms
- Require an additional level in the `memory hierarchy`
    - OS need a place to stash away portions of address space that currently aren't in great demand
    - In modern system, this role is usually served by a `hard disk drive`

1. Registers
2. Cache
3. Main Memory
4. Mass Storage (hard disk, tape, etc..)
> As we go down the hierarchy, the memory size increases, and speed decreases

### Single Large Address for a Process
- Always need to first arrange for the code or data to be in memory when before calling a function or accessing data
- To beyond just a `single process`
    - The addition of `swap space` allows the OS to support the illusion of a large virtual memory for multiple concurrently-running process

## Swap Space
- Reserve some space on the disk for moving pages back and forth
- OS need to remember to the swap space, in `page-sized unit`
- Same size as pages, called `swap blocks`

### Present Bit
- Add sosme machinery higher up in the system in order to support swapping pages to and from the disk
    - When the hardware looks in the PTE, it may find that the page is not <u>present</u> in physical memory
        - 1: present in physical memory
        - 0: present on disk

### What if memory is full?
- The OS like to page out pages to make room for the new pages the OS is about to bring in
    - The process of picking a page to kick out, or replace is known as `page-replacement` policy

## Page Fault
- Accessing page that is `not in physical memory`
    - If a page is not present and has been swapping disk, the OS need to swap the page into memory in order to service the page fault

### Page Fault Control Flow
- PTE used for data such as the pFN of the page for a disk 
> When the OS receives a page fault, it looks in the PTE and issues the request to disk

```c
// HARDWARE
VPN = (VirtualAddress & VPN_MASK) >> SHIFT
(Success, TlbEntry) = TLB_Lookup(VPN)
if (Success == True) // TLB Hit
    if(CanAccess(TlbEntry.ProtectBits == True))
        Offset = VirtualAddress & OFFSET_MASK
        PhysAddr = (TlbEntry.PFN << SHIFT) | OFFSET
        Register = AccessMemory(PhysAddr)
    else RaiseException(PROTECTION_FAULT)
else // TLB Miss
    PTEAddr = PTBR + (VPN * sizeof(PTE))
    PTE = AccessMemory(PTEAddr)
    if(PTE.Valid == False)
        RaiseException(SEGMENTATION_FAULT)
    else
        if(CanAccess(PTE.ProtectBits) == False)
            RaiseException(PROTECTION_FAULT)
        else if(PTE.PRESENT == True)
        // Assuming hardware-managed TLB
            TLB_Insert(VPN, PTE.PFN, PTE.ProtectBits)
            RetryInstruction()
        else if (PTE.Present == False)
            RaiseException(PAGE_FAULT)


// SOFTWARE
PFN = FindFreePhysicalPage()
if(PFN == -1) // no free page found
    PFN = EvictPage() // run replacement algorithm
DiskRead(PTE.DiskAddr, pfn) // sleep (waiting for I/O)
PTE.present = True // update page table with present
PTE.PFN = PFN // bit and translation (PFN)
RetryInstruction() // retry instruction
```
- The OS must find a physical frame for the `soon-be-faulted-in page` to reside within
- If there is no such page, waiting for the `replacememt algorithm` to run and kick some pages out of memory
- Memory accessed twice (When evicting, and when reading disk)

## When Replacements Really Occur
- OS waits until memory is entirely full, and only then replaces a page to make room for some other page
    - This is a little bit unrealistic, and there are many reason for the OS to keep a small portion of memory free more proactively
- Swap Daemon, Page Daemon
    - There are fewer than `LW pages` available, a background thread that is responsible for freeing memory runs
        > Low Watermark pages: number of pages must always be greater than the LW pages
        >> The thread will free memory until it reaches the HW pages
    - The thread evicts pages until there are `HW pages` available
        > High Watermark Pages

- Critical path: the path that must be taken during a proces
    - affects the overall performance of the program
- So swap daemon uses one write to evict all the pages
    - Huge efficiency boost

### Beyond Physical Memory: Policies
- <u>Memory pressure</u> forces the OS to start `paging out` pages to make to make room for actively-used pages
- Deciding which page to <u>evict</u> is encapsulated within the replacement policy of the OS

### Cache Management
- Goal in picking a replacement policy for this cache is to minimize the number of cache misses
- The number of cache hits and misses let us calculate the *average memory access time(AMAT)*
$$AMAT = (P_{hit} * T_M) + (P_{miss} * T_D)$$
- $T_M$: The cost of accessing memory
- $T_D$: The cost of accessing disk
- $P_{hit}$: The probability of finding the data item in the cache (a hit)
- $P_{miss}$: The probability of not finding the data in the cache (a miss)

## The Optimal Replacement Policy
- Leads to the fewest number of misses overall
    - Replaces the page that will be accessed <u>furthest in the future</u>
    - Resulting in the `fewest-possible` cache misses
- Serve only as a comparison point, to know how close we are to `perfect`
> Future is not known

$\text{Hit rate} = \frac{hits}{hits + misses}$

### A Simple Policy: FIFO
- Pages were placed in a queue when they enter the system
- When a replacement occurs, the page on the tail of the queue (the "`First-in`" pages) is evicted
    - It is simple to implement, but can't determine the importance of blocks

- Even though a page has been accessed a number of times, FIFO still kicks it out

#### Belady's Anomaly
- We would expect the cache hit rate to `increase` when the cache gets larger. But in this case, with FIFO, it gets worse

### Another Simple Policy: Random
- Picks a random page to replace under memory pressure
    - It doesn't really try to be intelligent in picking which blocks to evict
    - Random does depend entirely upon how luck <u>Random</u> gets in its choice

#### Random Performance
- Sometimes, `Random is as good as optimal`

### Using History
- Lean on the past and use <u>history</u>
    - Two type of historical information

- **LRU**, Recency: the more recently a page has been accessed, the more likely it will be accessed again
- **LFU**, Frequency: If a page has been accessed many times, it should not be replaced as it clearly has some value

#### LRU
- Replaces the least-recently-used page

## Worload Examples
### The No-Locality Workload
- Each reference is to a random page within the set of accessed pages
    - Workload accesses 100 unique pages over time
    - Choosing the next page to refer to at random
> When the cache is large enough to fit the entire workload, it also doesn't matter which policy you use

### The 80-20 Workload
- Exhibits locality: 80% of the `reference` are made to 20% of the page
- The remaining 20% of the `reference` are made to the remaining 80% of the pages
> LRU is more likely to hold onto the `hot pages`
- hot pages: frequently accessed pages

### The Looping Sequential
- Refer to 50 pages in sequence
- Starting at 0, then 1, ... up to page 49, and then we loop, repeating those accesses, for total of 10,000 accesses to 50 unique pages

> LRU then can use the Drop-Behind technique (Madvise)

## Implementing Historical Algorithms
- To keep track of which pages have been least-and-recently used, the system has to do some accounting work on **every memory reference**
    - `Add a little bit` of hardware support